{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn import datasets, plotting\n",
    "from nilearn.maskers import NiftiMapsMasker\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load fMRI Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[get_dataset_dir] Dataset found in /Users/anushamourshed/nilearn_data/development_fmri\n",
      "[get_dataset_dir] Dataset found in /Users/anushamourshed/nilearn_data/development_fmri/development_fmri\n",
      "[get_dataset_dir] Dataset found in /Users/anushamourshed/nilearn_data/development_fmri/development_fmri\n",
      "fMRI dataset loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Fetch the dataset (all subjects)\n",
    "development_dataset = datasets.fetch_development_fmri(n_subjects=None)\n",
    "print(\"fMRI dataset loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load the Brain Atlas (MSDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[get_dataset_dir] Dataset found in /Users/anushamourshed/nilearn_data/msdl_atlas\n",
      "MSDL has 39 ROIs, part of the following networks:\n",
      "['Aud', 'Aud', 'Striate', 'DMN', 'DMN', 'DMN', 'DMN', 'Occ post', 'Motor', 'R V Att', 'R V Att', 'R V Att', 'R V Att', 'Basal', 'L V Att', 'L V Att', 'L V Att', 'D Att', 'D Att', 'Vis Sec', 'Vis Sec', 'Vis Sec', 'Salience', 'Salience', 'Salience', 'Temporal', 'Temporal', 'Language', 'Language', 'Language', 'Language', 'Language', 'Cereb', 'Dors PCC', 'Cing-Ins', 'Cing-Ins', 'Cing-Ins', 'Ant IPS', 'Ant IPS']\n"
     ]
    }
   ],
   "source": [
    "# Fetch the MSDL atlas\n",
    "msdl_data = datasets.fetch_atlas_msdl()\n",
    "\n",
    "# Print basic info\n",
    "msdl_coords = msdl_data.region_coords\n",
    "n_regions = len(msdl_coords)\n",
    "\n",
    "print(f\"MSDL has {n_regions} ROIs, part of the following networks:\\n{msdl_data.networks}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize the Masker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masker initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set cache directory for Nilearn\n",
    "cache_dir = os.path.expanduser(\"~/nilearn_cache\")\n",
    "\n",
    "# Create the masker object\n",
    "masker = NiftiMapsMasker(\n",
    "    msdl_data.maps,\n",
    "    resampling_target=\"data\",\n",
    "    t_r=2,\n",
    "    detrend=True,\n",
    "    low_pass=0.1,\n",
    "    high_pass=0.01,\n",
    "    memory=cache_dir,  # Cache results\n",
    "    memory_level=1,\n",
    "    standardize=\"zscore_sample\",\n",
    "    standardize_confounds=True,\n",
    ").fit()\n",
    "print(\"Masker initialized successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extract Time Series and Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved time series...\n",
      "Total subjects: 155\n",
      "Total children: 122\n"
     ]
    }
   ],
   "source": [
    "# Check if saved time series exists\n",
    "if os.path.exists(\"fmri_time_series.npz\"):\n",
    "    print(\"Loading saved time series...\")\n",
    "    data = np.load(\"fmri_time_series.npz\", allow_pickle=True)\n",
    "    pooled_subjects = data[\"pooled_subjects\"]\n",
    "    children = data[\"children\"]\n",
    "    groups = data[\"groups\"]\n",
    "else:\n",
    "    print(\"Extracting time series (this will be saved for future use)...\")\n",
    "\n",
    "    children = []\n",
    "    pooled_subjects = []\n",
    "    groups = []  # Store 'child' or 'adult' labels\n",
    "\n",
    "    for func_file, confound_file, phenotype in zip(\n",
    "        development_dataset.func,\n",
    "        development_dataset.confounds,\n",
    "        development_dataset.phenotypic[\"Child_Adult\"],\n",
    "    ):\n",
    "        time_series = masker.transform(func_file, confounds=confound_file)\n",
    "        \n",
    "        pooled_subjects.append(time_series)  # Store all subjects\n",
    "\n",
    "        if phenotype == \"child\":\n",
    "            children.append(time_series)  # Store only children\n",
    "\n",
    "        groups.append(phenotype)  # Store class labels\n",
    "\n",
    "    # Save extracted time series and labels\n",
    "    np.savez(\"fmri_time_series.npz\", pooled_subjects=pooled_subjects, children=children, groups=groups)\n",
    "    print(\"Time series and labels saved successfully.\")\n",
    "\n",
    "print(f\"Total subjects: {len(pooled_subjects)}\")\n",
    "print(f\"Total children: {len(children)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compute Correlation Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing correlation matrices...\n",
      "Computing partial correlation matrices...\n",
      "Computing tangent matrices...\n",
      "Correlation matrix shape: (155, 39, 39)\n",
      "Partial correlation matrix shape: (155, 39, 39)\n",
      "Tangent matrix shape: (155, 39, 39)\n"
     ]
    }
   ],
   "source": [
    "# Compute correlation matrices\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "\n",
    "kinds = [\"correlation\", \"partial correlation\", \"tangent\"]\n",
    "connectivity_matrices = {}\n",
    "\n",
    "for kind in kinds:\n",
    "    print(f\"Computing {kind} matrices...\")\n",
    "    \n",
    "    # Compute correlation matrices\n",
    "    correlation_measure = ConnectivityMeasure(kind=kind, standardize=\"zscore_sample\")\n",
    "    connectivity_matrices[kind] = correlation_measure.fit_transform(pooled_subjects)\n",
    "    \n",
    "    # **Fix: Replace NaNs with 0 to prevent training issues**\n",
    "    connectivity_matrices[kind] = np.nan_to_num(connectivity_matrices[kind], nan=0.0)\n",
    "\n",
    "# Print shapes to verify correctness\n",
    "for kind in kinds:\n",
    "    print(f\"{kind.capitalize()} matrix shape: {connectivity_matrices[kind].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Convert Correlation Matrices to Graph Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Graph data successfully generated for selected thresholds and correlation methods.\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "thresholds = [0.05, 0.1, 0.3]  # Lower thresholds to avoid empty graphs\n",
    "\n",
    "# Convert labels: 'child' -> 1, 'adult' -> 0\n",
    "labels = np.array([1 if group == \"child\" else 0 for group in groups])\n",
    "\n",
    "def create_graph_data(correlation_matrices, labels, threshold):\n",
    "    \"\"\"\n",
    "    Converts correlation matrices into PyTorch Geometric graph data.\n",
    "    - Removes NaNs\n",
    "    - Applies threshold\n",
    "    - Ensures graphs have valid edges/nodes\n",
    "    \"\"\"\n",
    "    graph_data_list = []\n",
    "    \n",
    "    for i, matrix in enumerate(correlation_matrices):\n",
    "        # **Replace NaNs in adjacency matrix**\n",
    "        matrix = np.nan_to_num(matrix, nan=0.0)\n",
    "        \n",
    "        # **Apply threshold to filter connections**\n",
    "        adj_matrix = np.where(np.abs(matrix) > threshold, matrix, 0)\n",
    "        \n",
    "        # **Ensure matrix is not fully zero before converting to a graph**\n",
    "        if np.count_nonzero(adj_matrix) == 0:\n",
    "            print(f\"âš  Warning: Graph {i} for threshold {threshold} is completely empty. Skipping...\")\n",
    "            continue  \n",
    "\n",
    "        # Convert adjacency matrix to NetworkX graph\n",
    "        G = nx.from_numpy_array(adj_matrix)\n",
    "        \n",
    "        # **Skip empty graphs (no edges)**\n",
    "        if len(G.edges) == 0:  \n",
    "            print(f\"âš  Warning: Graph {i} for threshold {threshold} has no edges. Skipping...\")\n",
    "            continue  \n",
    "\n",
    "        edge_index = torch.tensor(list(G.edges), dtype=torch.long).t().contiguous()\n",
    "        edge_weight = torch.tensor([matrix[u, v] for u, v in G.edges], dtype=torch.float)\n",
    "\n",
    "        # **Ensure node features are not empty**\n",
    "        if len(G.nodes) == 0:\n",
    "            print(f\"âš  Warning: Graph {i} has no nodes! Skipping...\")\n",
    "            continue  \n",
    "\n",
    "        # Node features: Degree centrality\n",
    "        node_features = torch.tensor([[d] for _, d in G.degree()], dtype=torch.float)\n",
    "\n",
    "        # **Ensure labels are valid integers**\n",
    "        graph_data = Data(\n",
    "            x=node_features, \n",
    "            edge_index=edge_index, \n",
    "            edge_attr=edge_weight, \n",
    "            y=torch.tensor(labels[i], dtype=torch.long)  # 0 for adult, 1 for child\n",
    "        )\n",
    "        graph_data_list.append(graph_data)\n",
    "    \n",
    "    return graph_data_list\n",
    "\n",
    "# **Generate graphs for selected thresholds**\n",
    "graph_datasets = {\n",
    "    kind: {threshold: create_graph_data(connectivity_matrices[kind], labels, threshold) \n",
    "           for threshold in thresholds} \n",
    "    for kind in [\"correlation\", \"tangent\", \"partial correlation\"]\n",
    "}\n",
    "\n",
    "print(\"âœ… Graph data successfully generated for selected thresholds and correlation methods.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Splitting into Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data loaders created for all thresholds and correlation methods.\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Split datasets and create DataLoaders\n",
    "train_loaders = {}\n",
    "test_loaders = {}\n",
    "\n",
    "for kind in kinds:  # Loop over correlation methods\n",
    "    train_loaders[kind] = {}\n",
    "    test_loaders[kind] = {}\n",
    "\n",
    "    for threshold in thresholds:  # Loop over thresholds\n",
    "        train_graphs, test_graphs = train_test_split(graph_datasets[kind][threshold], test_size=0.2, random_state=42)\n",
    "        \n",
    "        train_loaders[kind][threshold] = DataLoader(train_graphs, batch_size=8, shuffle=True)\n",
    "        test_loaders[kind][threshold] = DataLoader(test_graphs, batch_size=8, shuffle=False)\n",
    "\n",
    "print(\"âœ… Data loaders created for all thresholds and correlation methods.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train GNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "class GNNClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GNNClassifier, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        \n",
    "        # Ensure edge_attr is not None\n",
    "        x = self.conv1(x, edge_index, edge_weight=edge_attr if edge_attr is not None else None)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_weight=edge_attr if edge_attr is not None else None)\n",
    "\n",
    "        # Ensure batch exists for pooling\n",
    "        batch = data.batch if hasattr(data, 'batch') else torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n",
    "        x = global_mean_pool(x, batch)  # Pool over nodes\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Define model parameters\n",
    "input_dim = 1  # Placeholder, update based on actual node features\n",
    "hidden_dim = 16\n",
    "output_dim = 2  # Binary classification (child vs. adult)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Initializing GNN model for correlation with threshold 0.05...\n",
      "ðŸš€ Initializing GNN model for correlation with threshold 0.1...\n",
      "ðŸš€ Initializing GNN model for correlation with threshold 0.3...\n",
      "ðŸš€ Initializing GNN model for partial correlation with threshold 0.05...\n",
      "ðŸš€ Initializing GNN model for partial correlation with threshold 0.1...\n",
      "ðŸš€ Initializing GNN model for partial correlation with threshold 0.3...\n",
      "ðŸš€ Initializing GNN model for tangent with threshold 0.05...\n",
      "ðŸš€ Initializing GNN model for tangent with threshold 0.1...\n",
      "ðŸš€ Initializing GNN model for tangent with threshold 0.3...\n"
     ]
    }
   ],
   "source": [
    "# Device setup for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dictionary to store models for all combinations\n",
    "models = {}\n",
    "\n",
    "for kind in kinds:  # Loop over correlation methods\n",
    "    models[kind] = {}\n",
    "    for threshold in thresholds:  # Loop over thresholds\n",
    "        print(f\"ðŸš€ Initializing GNN model for {kind} with threshold {threshold}...\")\n",
    "        \n",
    "        # Create and move model to device\n",
    "        model = GNNClassifier(input_dim, hidden_dim, output_dim).to(device)\n",
    "        models[kind][threshold] = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train_model(model, train_loader, epochs=50, lr=0.001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.NLLLoss()  # Negative log likelihood loss for classification\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(batch)\n",
    "            loss = criterion(output, batch.y)\n",
    "            \n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "    \n",
    "    print(\"âœ… Training complete!\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Training GNN on correlation graphs with threshold 0.05...\n",
      "\n",
      "Epoch 0, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "âœ… Training complete!\n",
      "\n",
      "\n",
      "ðŸš€ Training GNN on correlation graphs with threshold 0.1...\n",
      "\n",
      "Epoch 0, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "âœ… Training complete!\n",
      "\n",
      "\n",
      "ðŸš€ Training GNN on correlation graphs with threshold 0.3...\n",
      "\n",
      "Epoch 0, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "âœ… Training complete!\n",
      "\n",
      "\n",
      "ðŸš€ Training GNN on partial correlation graphs with threshold 0.05...\n",
      "\n",
      "Epoch 0, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "âœ… Training complete!\n",
      "\n",
      "\n",
      "ðŸš€ Training GNN on partial correlation graphs with threshold 0.1...\n",
      "\n",
      "Epoch 0, Loss: 0.5125\n",
      "Epoch 10, Loss: 0.4761\n",
      "Epoch 20, Loss: 0.4786\n",
      "Epoch 30, Loss: 0.4814\n",
      "Epoch 40, Loss: 0.4814\n",
      "âœ… Training complete!\n",
      "\n",
      "\n",
      "ðŸš€ Training GNN on partial correlation graphs with threshold 0.3...\n",
      "\n",
      "Epoch 0, Loss: 0.5538\n",
      "Epoch 10, Loss: 0.4935\n",
      "Epoch 20, Loss: 0.4853\n",
      "Epoch 30, Loss: 0.4821\n",
      "Epoch 40, Loss: 0.4924\n",
      "âœ… Training complete!\n",
      "\n",
      "\n",
      "ðŸš€ Training GNN on tangent graphs with threshold 0.05...\n",
      "\n",
      "Epoch 0, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "âœ… Training complete!\n",
      "\n",
      "\n",
      "ðŸš€ Training GNN on tangent graphs with threshold 0.1...\n",
      "\n",
      "Epoch 0, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "âœ… Training complete!\n",
      "\n",
      "\n",
      "ðŸš€ Training GNN on tangent graphs with threshold 0.3...\n",
      "\n",
      "Epoch 0, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "âœ… Training complete!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 50  # Adjust as needed\n",
    "\n",
    "for kind in kinds:  # Loop over correlation methods\n",
    "    for threshold in thresholds:  # Loop over thresholds\n",
    "        print(f\"\\nðŸš€ Training GNN on {kind} graphs with threshold {threshold}...\\n\")\n",
    "\n",
    "        # Get model & train loader\n",
    "        model = models[kind][threshold]\n",
    "        train_loader = train_loaders[kind][threshold]\n",
    "\n",
    "        # Train the model\n",
    "        train_model(model, train_loader, epochs=epochs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.9 ('capstone')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61c12dabc53b3c38026121b5b57a874e2c527979eaaf65ce1b82c6f270ed05c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
